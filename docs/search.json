[
  {
    "objectID": "posts/welcome_post/index.html",
    "href": "posts/welcome_post/index.html",
    "title": "Welcome to my Blog",
    "section": "",
    "text": "Hey there! üëã\nI‚Äôve finally set up this blog, and I‚Äôm excited to start sharing my thoughts, projects, and discoveries here. Stay tuned for upcoming posts‚ÄîI‚Äôll be diving into topics that interest me, from tech and research to random things I find fascinating.\nLooking forward to this journey. Let‚Äôs see where it goes!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sina Schlegel",
    "section": "",
    "text": "I am an undergraduate Economics student at the University of St.¬†Gallen, with a strong foundation in mathematics and passion for data science. Currently, I work as a Student Assistant at HSG‚Äôs Chair of Mathematics and have experience in Python, C++, R, and LaTeX. I am passionate about combining quantitative analysis with economic insights and enjoy working on projects that bridge these fields.\nThank you very much for visiting and feel free to explore posts on my Blog using the navigation bar on the top right."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Row-Column Game\n\n\n\n\n\n\nGame Theory\n\n\nPython\n\n\n\n\n\n\n\n\n\nDec 11, 2025\n\n\nSina Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nWater Level Forecast of Lake Constance\n\n\n\n\n\n\nTime Series Analysis\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nFeb 25, 2025\n\n\nSina Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\nSina Schlegel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Lake_Constance/index.html",
    "href": "posts/Lake_Constance/index.html",
    "title": "Water Level Forecast of Lake Constance",
    "section": "",
    "text": "Water levels in large lakes are subject to complex dynamics driven by meteorological conditions, hydrology, and long-term climate trends. For Lake Constance, these fluctuations have direct operational consequences. During periods of high water levels, steeper entry and exit angles at ferry ramps complicate boarding and disembarking, particularly for low-profile vehicles that may even be restricted to avoid damage. Conversely, low water levels can reduce cargo capacity, require stricter loading rules, and increase delays and fuel consumption due to suboptimal routing and load distribution.\nThese operational frictions lead to customer dissatisfaction, lost revenue, and higher overall costs for ferry companies operating on Lake Constance. Reliable short-term water level forecasts are therefore highly valuable, as they enable better planning of schedules, loading strategies, and potential service suspensions.\nIn this project, we develop models to predict next-day water levels at Lake Constance using a comprehensive set of meteorological, hydrological, and climate-related features. Formally, let\n\n\\(y_t\\) denote the observed lake level on day \\(t\\), and\n\n\\({x}_t\\) the feature vector observed on day \\(t\\).\n\nThe one-step-ahead forecasting problem is to construct a model \\(f(\\cdot)\\) such that\n\\[\n\\hat{y}_{t+1} = f(x_t), \\qquad t = 1,\\dots,T-1,\n\\]\nand to evaluate its out-of-sample performance on a hold-out period.\nWe compare several classes of models:\n\na classical ARMA time-series model,\n\nregularized linear regression (Ridge and Lasso),\n\na non-linear Random Forest, and\n\na feed-forward Neural Network.\n\nPerformance is assessed via the root mean squared error (RMSE),\n\\[\n\\text{RMSE} =\n\\sqrt{\\frac{1}{N}\\sum_{t=1}^{N}(y_t - \\hat{y}_t)^2},\n\\]\nand benchmarked against a naive persistence model (_{t+1}^{} = y_t). We find that modern machine learning methods and regularized linear models substantially improve upon the naive baseline, with Ridge regression achieving the lowest RMSE in our experiments."
  },
  {
    "objectID": "posts/Lake_Constance/index.html#sec-introduction",
    "href": "posts/Lake_Constance/index.html#sec-introduction",
    "title": "Water Level Forecast of Lake Constance",
    "section": "",
    "text": "Water levels in large lakes are subject to complex dynamics driven by meteorological conditions, hydrology, and long-term climate trends. For Lake Constance, these fluctuations have direct operational consequences. During periods of high water levels, steeper entry and exit angles at ferry ramps complicate boarding and disembarking, particularly for low-profile vehicles that may even be restricted to avoid damage. Conversely, low water levels can reduce cargo capacity, require stricter loading rules, and increase delays and fuel consumption due to suboptimal routing and load distribution.\nThese operational frictions lead to customer dissatisfaction, lost revenue, and higher overall costs for ferry companies operating on Lake Constance. Reliable short-term water level forecasts are therefore highly valuable, as they enable better planning of schedules, loading strategies, and potential service suspensions.\nIn this project, we develop models to predict next-day water levels at Lake Constance using a comprehensive set of meteorological, hydrological, and climate-related features. Formally, let\n\n\\(y_t\\) denote the observed lake level on day \\(t\\), and\n\n\\({x}_t\\) the feature vector observed on day \\(t\\).\n\nThe one-step-ahead forecasting problem is to construct a model \\(f(\\cdot)\\) such that\n\\[\n\\hat{y}_{t+1} = f(x_t), \\qquad t = 1,\\dots,T-1,\n\\]\nand to evaluate its out-of-sample performance on a hold-out period.\nWe compare several classes of models:\n\na classical ARMA time-series model,\n\nregularized linear regression (Ridge and Lasso),\n\na non-linear Random Forest, and\n\na feed-forward Neural Network.\n\nPerformance is assessed via the root mean squared error (RMSE),\n\\[\n\\text{RMSE} =\n\\sqrt{\\frac{1}{N}\\sum_{t=1}^{N}(y_t - \\hat{y}_t)^2},\n\\]\nand benchmarked against a naive persistence model (_{t+1}^{} = y_t). We find that modern machine learning methods and regularized linear models substantially improve upon the naive baseline, with Ridge regression achieving the lowest RMSE in our experiments."
  },
  {
    "objectID": "posts/Lake_Constance/index.html#sec-methodology",
    "href": "posts/Lake_Constance/index.html#sec-methodology",
    "title": "Water Level Forecast of Lake Constance",
    "section": "2 Methodology",
    "text": "2 Methodology\n\n2.1 Problem Setup\nWe focus on daily water levels measured in Constance, one of the standard gauging stations at Lake Constance. Let \\(\\{y_t\\}_{t=1}^T\\) denote the water level time series (in cm). The goal is to forecast \\(y_{t+1}\\) using information available up to day \\(t\\).\nThe general prediction framework can be written as\n\\[\ny_{t+1} = f(x_t) + \\varepsilon_{t+1},\n\\]\nwhere \\(x_t\\) contains contemporaneous and lagged features and \\(\\varepsilon_{t+1}\\) is an error term with mean zero.\n\n\n2.2 Data\nWe combine several datasets to construct \\({x}_t\\):\n\nHydrological data\n\nDaily water levels of Lake Constance (station Constance) from 1924 to 2023.\n\nWeekly groundwater levels from a nearby station.\n\nMeteorological data (daily)\n\nhumidity, mean temperature, precipitation,\n\nsnow depth, sea-level pressure,\n\nsunshine duration, global radiation, wind speed.\n\nLunar data\n\nDaily moon illumination from NASA‚Äôs Horizons system, representing the fraction of the moon‚Äôs disk that is illuminated.\n\nClimate change data\n\nAnnual mean surface temperature change for Switzerland, used as a proxy for long-term climate trends.\n\n\nAll datasets are converted to CSV format, cleaned, and merged by date to form a single panel. The final modeling period spans 1973‚Äì2023, providing a long history with clear seasonal and interannual patterns.\n\n\n2.3 Preprocessing\nSeveral preprocessing steps are required before modeling:\n\n2.3.1 Temporal alignment and target shift\nBecause we want to predict tomorrow‚Äôs water level using today‚Äôs information, the target series is shifted:\n\\[\n\\tilde{y}_t = y_{t+1}, \\qquad t = 1,\\dots,T-1,\n\\]\nso that each row \\((x_t, \\tilde{y}_t)\\) encodes features at day \\(t\\) and the corresponding water level on day (t+1).\n\n\n2.3.2 Handling missing values and outliers\n\nGroundwater (weekly) and climate change (annual) data are forward-filled to match the daily resolution.\n\nMissing values in weather variables (e.g., snow depth, sunshine, wind speed, radiation) are imputed using **seasonal monthly means, preserving seasonal patterns.\n\nOutliers are identified via time-series plots and corrected using a 30-day rolling mean where appropriate, relying only on historical/current information to avoid data leakage.\n\n\n\n2.3.3 Train‚Äìtest split\nTo ensure a realistic evaluation, we use a temporal split:\n\nTraining set: data up to 2017 (inclusive).\n\nTest set: 2018‚Äì2023.\n\nThis respects the time order and avoids training on future information.\n\n\n2.3.4 Log transformation and feature engineering\n\nExploratory analysis shows that water levels are right-skewed and approximately log-normally distributed. A log transformation stabilizes variance:\n\\[\nz_t = \\log(y_t).\n\\]\nTo capture temporal dependence, we add lagged levels as features:\n\\[\nz_{t-1}, z_{t-2}, \\dots, z_{t-7},\n\\]\nrepresenting the last seven days of water levels. Because these lags would be known at prediction time, they do not introduce look-ahead bias.\nFor models that can exploit seasonality directly (Random Forest, Neural Network), we additionally encode calendar effects via cyclical features, e.g.\n\\[\n\\begin{aligned}\n\\text{month\\_sin}_t &= \\sin\\left( 2\\pi \\cdot \\frac{\\text{month}(t)}{12} \\right), \\\\\n\\text{month\\_cos}_t &= \\cos\\left( 2\\pi \\cdot \\frac{\\text{month}(t)}{12} \\right),\n\\end{aligned}\n\\]\nand analogous terms for week-of-year, half-year, etc.\nFor linear models and ARMA, we work on deseasonalized versions of the series to meet stationarity and linearity assumptions (see below).\n\n\n\n2.4 Time-Series Decomposition\nFourier analysis and seasonal plots reveal strong annual and semi-annual components in the water level series, but no pronounced long-term trend. For models that require approximate stationarity (ARMA) or that may be dominated by spurious seasonal correlations (linear regression), we:\n\nRemove seasonal components from the target and from features that share similar seasonality.\n\nDetrend only those features exhibiting strong trends unrelated to the target (notably the climate change indicator).\n\nStationarity is confirmed using Augmented Dickey‚ÄìFuller (ADF) and KPSS tests on the deseasonalized target."
  },
  {
    "objectID": "posts/Lake_Constance/index.html#sec-models",
    "href": "posts/Lake_Constance/index.html#sec-models",
    "title": "Water Level Forecast of Lake Constance",
    "section": "3 Models",
    "text": "3 Models\nWe compare four main model classes.\n\n3.1 ARMA\nThe ARMA model treats deseasonalized log water levels as a univariate time series \\(\\tilde{z}_t\\) and assumes\n\\[\n\\tilde{z}_t = \\mu + \\sum_{i=1}^{p} \\phi_i \\tilde{z}_{t-i}\n+ \\sum_{j=1}^{q} \\theta_j \\varepsilon_{t-j}\n+ \\varepsilon_t,\n\\qquad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2).\n\\]\nWe perform a grid search over orders \\(p\\) and \\(q\\) on the training set using time-series cross-validation, selecting the combination that minimizes RMSE. The optimal configuration is ARMA(4,3). Forecasts are generated in a rolling fashion, always using the most recent observed (or previously predicted) values.\n\n\n3.2 Regularized Linear Regression (Ridge and Lasso)\nAfter deseasonalization and standardization, we model the relationship between features (_t) and target \\(\\tilde{z}_t\\) via\n\\[\n\\tilde{z}_t = \\beta_0 + x_t^\\top \\boldsymbol{\\beta} + \\varepsilon_t,\n\\]\nwith two different regularization schemes:\n\nRidge regression solves\n\\[\n\\min_{\\beta_0,\\boldsymbol{\\beta}}\n\\sum_{t} (\\tilde{z}_t - \\beta_0 - \\mathbf{x}_t^\\top \\boldsymbol{\\beta})^2\n+ \\lambda \\lVert \\boldsymbol{\\beta} \\rVert_2^2,\n\\]\nLasso regression solves\n\\[\n\\min_{\\beta_0,\\boldsymbol{\\beta}}\n\\sum_{t} (\\tilde{z}_t - \\beta_0 - \\mathbf{x}_t^\\top \\boldsymbol{\\beta})^2\n+ \\lambda \\lVert \\boldsymbol{\\beta} \\rVert_1.\n\\]\n\nThe regularization parameter \\(\\lambda\\) is selected via grid search with TimeSeriesSplit cross-validation. Ridge primarily shrinks coefficients, while Lasso additionally performs variable selection by driving some coefficients to zero.\n\n\n3.3 Random Forest\nRandom Forest is an ensemble of decision trees. Each tree (f^{(b)}) is fitted on a bootstrap sample of the training data, considering a random subset of features at each split. The final prediction is the average\n\\[\n\\hat{z}_{t+1}^{\\text{RF}} = \\frac{1}{B} \\sum_{b=1}^{B} f^{(b)}({x}_t).\n\\]\nHyperparameters such as the number of trees, maximum depth, and minimum samples per split are tuned via RandomizedSearchCV with time-series splits. After tuning, we train a larger forest (e.g.¬†(B = 500) trees) using the best configuration. Random Forest additionally provides feature importance scores, which we exploit to interpret the drivers of predictability.\n\n\n3.4 Neural Network\nFor the neural network, we use a multi-layer perceptron (MLP) implemented via scikit-learn‚Äôs MLPRegressor. The MLP models a non-linear mapping\n\\[\n\\hat{z}_{t+1}^{\\text{NN}} = f_{\\text{NN}}(\\mathbf{x}_t; \\Theta),\n\\]\nwhere \\(\\Theta\\) denotes all weights and biases. The chosen architecture has three hidden layers with 32, 64, and 32 neurons, respectively, ReLU activations, and the Adam optimizer with learning rate \\(10^{-4}\\). An \\(L_2\\) penalty with \\(\\alpha = 0.01\\) controls overfitting.\nHyperparameters (layer sizes, batch size, learning rate, regularization) are tuned via RandomizedSearchCV. We assess feature importance using permutation importance, which measures the increase in prediction error when a feature is randomly shuffled."
  },
  {
    "objectID": "posts/Lake_Constance/index.html#sec-results",
    "href": "posts/Lake_Constance/index.html#sec-results",
    "title": "Water Level Forecast of Lake Constance",
    "section": "4 Results",
    "text": "4 Results\n\n4.1 Forecast Accuracy\nTable 1 reports RMSE values for all models on the test set (2018‚Äì2023), along with the naive ‚Äúyesterday = today‚Äù benchmark.\nTable 1: Out-of-sample RMSE for next-day water level forecasts (cm).\n\n\n\nModel\nRMSE\n\n\n\n\nNaive (\\(y_t\\))\n2.85\n\n\nARMA(4,3)\n2.12\n\n\nRidge Regression\n1.40\n\n\nLasso Regression\n1.43\n\n\nRandom Forest\n2.04\n\n\nNeural Network\n1.96\n\n\n\nRidge regression achieves the lowest RMSE (1.40 cm), followed closely by Lasso and the neural network. Both Random Forest and ARMA substantially improve over the naive baseline, but remain less accurate than the best linear and neural models.\n\n\n4.2 Interpretation of Drivers\nAcross Random Forest and Neural Network models, lagged water levels, in particular the first lag (z_{t-1}), dominate feature importance. In the Random Forest, lag 1 alone accounts for more than 99% of the total importance, confirming that water levels are highly persistent. When lag 1 is excluded to inspect secondary drivers, precipitation and additional lagged levels emerge as the most influential features, while snow depth contributes little additional information.\nPermutation importance for the neural network confirms the same pattern:\n\nlagged levels are crucial,\n\nprecipitation plays a supporting role,\n\nother weather variables, lunar features, and long-term climate indicators appear less relevant for one-day-ahead forecasts.\n\n\n\n4.3 Comparison to Baseline\nRelative to the naive model (RMSE 2.85 cm), the best model (Ridge, 1.40 cm) reduces the typical forecast error by roughly 50%. Even the ARMA model improves RMSE by about 25%. In operational terms, this tighter error band enables ferry operators to:\n\nbetter anticipate when ramps may become too steep or too shallow,\n\nadjust loading strategies in advance, and\n\ncommunicate potential disruptions earlier to customers."
  },
  {
    "objectID": "posts/Lake_Constance/index.html#sec-conclusion",
    "href": "posts/Lake_Constance/index.html#sec-conclusion",
    "title": "Water Level Forecast of Lake Constance",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis study investigates next-day water level forecasts for Lake Constance using a rich feature set combining hydrological, meteorological, lunar, and climate change data. After careful preprocessing, feature engineering, and model tuning, we compare ARMA, regularized linear regressions, Random Forests, and a neural network.\nOur main findings are:\n\nLagged water levels are by far the most important predictors of tomorrow‚Äôs level; one-day persistence dominates the dynamics.\n\nPrecipitation and additional lags provide useful supplementary information, while other variables play a comparatively minor role for one-day-ahead predictions.\n\nRidge regression achieves the best overall performance, slightly outperforming Lasso and the neural network, and clearly improving upon ARMA and Random Forest.\n\nAll advanced models significantly outperform a naive persistence benchmark, reducing RMSE by up to 50%.\n\nFrom a practical perspective, these results demonstrate that relatively simple, interpretable models such as Ridge regression can already deliver highly accurate forecasts that are suitable for operational decision-making. Future work could extend the horizon to multi-day forecasts, incorporate probabilistic prediction intervals, or explore spatially distributed models that jointly consider multiple gauging stations around Lake Constance."
  },
  {
    "objectID": "posts/Row_Column_Game/index.html",
    "href": "posts/Row_Column_Game/index.html",
    "title": "Row-Column Game",
    "section": "",
    "text": "In this post, I explore the design of intelligent decision-making strategies for the Row‚ÄìColumn Game, a turn-based two-player game defined by a surprisingly strategic rule:\nAfter the first move, each player must select a cell in the same row or column as the opponent‚Äôs previous move.\nThe game is played on an \\(n \\times n\\) grid of positive integers. Each chosen cell adds to the player‚Äôs score and is removed from the board.\nThe game continues until no legal moves remain.\nAlthough the rules are simple, the game produces a rich space of decisions: sometimes aggressively taking a high-value cell is optimal, but often it hands your opponent a devastating reply.\nThis makes the game a perfect playground for experimenting with different kinds of AI strategies.\n‚Äì"
  },
  {
    "objectID": "posts/Row_Column_Game/index.html#sec-introduction",
    "href": "posts/Row_Column_Game/index.html#sec-introduction",
    "title": "Row-Column Game",
    "section": "",
    "text": "In this post, I explore the design of intelligent decision-making strategies for the Row‚ÄìColumn Game, a turn-based two-player game defined by a surprisingly strategic rule:\nAfter the first move, each player must select a cell in the same row or column as the opponent‚Äôs previous move.\nThe game is played on an \\(n \\times n\\) grid of positive integers. Each chosen cell adds to the player‚Äôs score and is removed from the board.\nThe game continues until no legal moves remain.\nAlthough the rules are simple, the game produces a rich space of decisions: sometimes aggressively taking a high-value cell is optimal, but often it hands your opponent a devastating reply.\nThis makes the game a perfect playground for experimenting with different kinds of AI strategies.\n‚Äì"
  },
  {
    "objectID": "posts/Row_Column_Game/index.html#sec-advantage",
    "href": "posts/Row_Column_Game/index.html#sec-advantage",
    "title": "Row-Column Game",
    "section": "2 Understanding Initiative and Positional Advantage",
    "text": "2 Understanding Initiative and Positional Advantage\nBefore considering individual strategies, it is important to understand the fundamental asymmetries built into the Row‚ÄìColumn Game. Although the rules are simple and both players face the same restrictions, the first move shapes the entire structure of the remaining game, and often creates a natural advantage for Player 1. This advantage depends on the size of the board.\nTo see why, imagine two types of boards:\n\nOdd number of cells (n % 2 &gt; 0), e.g.¬†3√ó3, 5√ó5\n\nEven number of cells (n % 2 = 0), e.g.¬†4√ó4, 6√ó6\n\nThese two cases behave very differently.\n\n2.1 Odd board sizes: Player 1 gets ‚Äúone extra life‚Äù\nIn a 3√ó3 game there are 9 total moves.\nNo matter what happens, Player 1 gets 5 moves and Player 2 gets only 4.\nThis alone is already a mathematical advantage, even if all cell values were equal,\nbut in real boards values vary, so Player 1 also gets one more chance to grab something valuable.\nIn other words:\non odd-sized boards, Player 2 starts the game slightly doomed unless they play with a real plan.\nThis also means Player 2 must actively try to:\n\nforce an early endgame,\n\ncollapse the state into an isolated row/column,\n\nor otherwise ‚Äútrap‚Äù the game into a situation where Player 1‚Äôs extra move becomes worthless.\n\nPlayer 1, on the other hand, must avoid being funneled into a prematurely dead state where that 5th move never materializes.\n\n\n2.2 Even board sizes: Player 1 wants to end the game early\nThe dynamic flips for even-sized boards.\nIn a 4√ó4 game there are 16 moves, an even number, meaning:\n\nIf both players keep the game alive, Player 2 gets the last move.\nBut if Player 1 can pinch the state into an isolated sub-board,\nforcing a premature ‚Äúgame over,‚Äù\nthen Player 1 steals the final move instead.\n\nSo the incentive structure becomes:\n\nPlayer 1: look for opportunities to end the game early\n\nPlayer 2: avoid collapse, keep the game alive, stretch the move count back to even\n\nThis interplay of ‚Äúwho wants the game to continue‚Äù is a huge part of strategy design.\n\n\n2.3 Why detecting isolated sequences matters\nAn isolated sequence (or forced end block) is any row‚Äìcolumn region where only one or two legal moves remain, and\nneither player can escape to the rest of the board.\nIf your algorithm cannot detect these situations, then:\n\nPlayer 2 will never overcome the parity disadvantage on odd boards, and\n\nPlayer 1 will miss opportunities to secure an extra move on even boards**\n\nIn other words, without detecting isolation states, no strategy can meaningfully overcome or exploit the fundamental mathematical structure of the game.\nAny competitive AI must therefore recognize when the game is about to collapse\nand adjust decisions accordingly.\nThis parity insight guided the design of our strategies and also explains many\npatterns seen later in the simulation results."
  }
]